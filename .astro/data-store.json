[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.13.5","content-config-digest","993cefaa27a7afe3","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://tam11a.dev\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"prefetch\":{\"defaultStrategy\":\"viewport\"},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false},\"legacy\":{\"collections\":false}}","featured",["Map",11,12,59,60,111,112,171,172,264,265],"ktu_pqys",{"id":11,"data":13,"body":21,"filePath":22,"digest":23,"rendered":24},{"index":14,"name":15,"logo":16,"featured":16,"role":17,"type":18,"timeline":19,"description":20},6,"KTU PYQs Fetcher","/work/pyq.svg","Developer","Open Source","2025 - Present","üìò KTU PYQs ‚Äì A fast and minimal web app to fetch Kerala Technological University Previous Year Question Papers using subject codes (e.g., CS301, MA201). Built with Next.js + Tailwind CSS, hosted on Vercel.","# üìò KTU PYQs\r\n\r\nA simple and lightweight web app to fetch **KTU Previous Year Question Papers (PYQs)** instantly using just the **subject code** (e.g., `CS301`, `MA201`).\r\n\r\nüîó Live Website: [ktu-pyqs.vercel.app](https://ktu-pyqs.vercel.app)\r\n\r\n---\r\n\r\n## ‚ú® Features\r\n\r\n* üéØ Fetch KTU PYQs using **subject codes**\r\n* ‚ö° Fast & minimal UI (no extra clutter)\r\n* üìÇ Organized by **semester, branch, and subject**\r\n* üåê Hosted on **Vercel** for reliability and speed\r\n* üì± Fully responsive ‚Äì works on desktop & mobile\r\n\r\n---\r\n\r\n## üõ†Ô∏è Tech Stack\r\n\r\n* **Frontend:** Next.js / React\r\n* **Styling:** Tailwind CSS\r\n* **Hosting:** Vercel\r\n\r\n---\r\n\r\n## üìå Usage\r\n\r\n1. Enter your **subject code** (e.g., `CS301`).\r\n2. Hit **Search**.\r\n3. Instantly get the corresponding **Previous Year Question Papers**.\r\n\r\n---\r\n\r\n## üîÆ Upcoming Features\r\n\r\n* üìë **Solved PYQs Fetcher** (Scraping from trusted sources)\r\n* üîç Search by **subject name** instead of just codes\r\n* ‚≠ê Bookmark & save frequently used papers\r\n* üåô Dark Mode support\r\n\r\n---\r\n\r\n## ü§ù Contributing\r\n\r\nContributions are welcome! If you‚Äôd like to fix a bug, improve the UI, or add features:\r\n\r\n1. Fork the repo\r\n2. Create a new branch (`feature/my-feature`)\r\n3. Commit changes\r\n4. Push and open a PR üéâ\r\n\r\n---\r\n\r\n## üìú License\r\n\r\nThis project is licensed under the **MIT License** ‚Äì feel free to use and modify.\r\n\r\n---\r\n\r\n‚ú® Built with ‚ù§Ô∏è by [Sreehari](https://github.com/Sree14hari)","src/pages/work/ktu_pqys.md","c26cabf19a9e0cae",{"html":25,"metadata":26},"\u003Ch1 id=\"-ktu-pyqs\">üìò KTU PYQs\u003C/h1>\n\u003Cp>A simple and lightweight web app to fetch \u003Cstrong>KTU Previous Year Question Papers (PYQs)\u003C/strong> instantly using just the \u003Cstrong>subject code\u003C/strong> (e.g., \u003Ccode>CS301\u003C/code>, \u003Ccode>MA201\u003C/code>).\u003C/p>\n\u003Cp>üîó Live Website: \u003Ca href=\"https://ktu-pyqs.vercel.app\">ktu-pyqs.vercel.app\u003C/a>\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"-features\">‚ú® Features\u003C/h2>\n\u003Cul>\n\u003Cli>üéØ Fetch KTU PYQs using \u003Cstrong>subject codes\u003C/strong>\u003C/li>\n\u003Cli>‚ö° Fast &#x26; minimal UI (no extra clutter)\u003C/li>\n\u003Cli>üìÇ Organized by \u003Cstrong>semester, branch, and subject\u003C/strong>\u003C/li>\n\u003Cli>üåê Hosted on \u003Cstrong>Vercel\u003C/strong> for reliability and speed\u003C/li>\n\u003Cli>üì± Fully responsive ‚Äì works on desktop &#x26; mobile\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Ch2 id=\"Ô∏è-tech-stack\">üõ†Ô∏è Tech Stack\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Frontend:\u003C/strong> Next.js / React\u003C/li>\n\u003Cli>\u003Cstrong>Styling:\u003C/strong> Tailwind CSS\u003C/li>\n\u003Cli>\u003Cstrong>Hosting:\u003C/strong> Vercel\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Ch2 id=\"-usage\">üìå Usage\u003C/h2>\n\u003Col>\n\u003Cli>Enter your \u003Cstrong>subject code\u003C/strong> (e.g., \u003Ccode>CS301\u003C/code>).\u003C/li>\n\u003Cli>Hit \u003Cstrong>Search\u003C/strong>.\u003C/li>\n\u003Cli>Instantly get the corresponding \u003Cstrong>Previous Year Question Papers\u003C/strong>.\u003C/li>\n\u003C/ol>\n\u003Chr>\n\u003Ch2 id=\"-upcoming-features\">üîÆ Upcoming Features\u003C/h2>\n\u003Cul>\n\u003Cli>üìë \u003Cstrong>Solved PYQs Fetcher\u003C/strong> (Scraping from trusted sources)\u003C/li>\n\u003Cli>üîç Search by \u003Cstrong>subject name\u003C/strong> instead of just codes\u003C/li>\n\u003Cli>‚≠ê Bookmark &#x26; save frequently used papers\u003C/li>\n\u003Cli>üåô Dark Mode support\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Ch2 id=\"-contributing\">ü§ù Contributing\u003C/h2>\n\u003Cp>Contributions are welcome! If you‚Äôd like to fix a bug, improve the UI, or add features:\u003C/p>\n\u003Col>\n\u003Cli>Fork the repo\u003C/li>\n\u003Cli>Create a new branch (\u003Ccode>feature/my-feature\u003C/code>)\u003C/li>\n\u003Cli>Commit changes\u003C/li>\n\u003Cli>Push and open a PR üéâ\u003C/li>\n\u003C/ol>\n\u003Chr>\n\u003Ch2 id=\"-license\">üìú License\u003C/h2>\n\u003Cp>This project is licensed under the \u003Cstrong>MIT License\u003C/strong> ‚Äì feel free to use and modify.\u003C/p>\n\u003Chr>\n\u003Cp>‚ú® Built with ‚ù§Ô∏è by \u003Ca href=\"https://github.com/Sree14hari\">Sreehari\u003C/a>\u003C/p>",{"headings":27,"localImagePaths":51,"remoteImagePaths":52,"frontmatter":53,"imagePaths":58},[28,32,36,39,42,45,48],{"depth":29,"slug":30,"text":31},1,"-ktu-pyqs","üìò KTU PYQs",{"depth":33,"slug":34,"text":35},2,"-features","‚ú® Features",{"depth":33,"slug":37,"text":38},"Ô∏è-tech-stack","üõ†Ô∏è Tech Stack",{"depth":33,"slug":40,"text":41},"-usage","üìå Usage",{"depth":33,"slug":43,"text":44},"-upcoming-features","üîÆ Upcoming Features",{"depth":33,"slug":46,"text":47},"-contributing","ü§ù Contributing",{"depth":33,"slug":49,"text":50},"-license","üìú License",[],[],{"index":14,"name":15,"logo":16,"featured":16,"role":17,"type":18,"timeline":19,"ref_urls":54,"description":20,"layout":57},[55],{"name":15,"url":56},"https://ktu-pyqs.vercel.app","@layouts/markdown.astro",[],"nlp_disaster",{"id":59,"data":61,"body":69,"filePath":70,"digest":71,"rendered":72},{"index":62,"name":63,"logo":64,"featured":64,"role":65,"type":66,"timeline":67,"description":68},5,"NLP with Disaster Tweets","/work/nlp.svg","Competitor","Competition","2025","This particular challenge is perfect for data scientists looking to get started with Natural Language Processing. The competition dataset is not too big, and even if you don‚Äôt have much personal computing power, you can do all of the work in our free, no-setup, Jupyter Notebooks environment called Kaggle Notebooks.","# Kaggle: NLP with Disaster Tweets (Top 10 Finish)\r\n\r\n[](https://www.kaggle.com/c/nlp-getting-started)\r\n[](https://www.kaggle.com/c/nlp-getting-started/leaderboard)\r\n[](https://www.python.org)\r\n[](https://pytorch.org/)\r\n[](https://github.com/huggingface/transformers)\r\n\r\n## Project Overview\r\n\r\nThis repository contains the code and methodology for the Kaggle competition \"[Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started)\". The goal is to build a machine learning model that can determine whether a given tweet is about a real disaster or not.\r\n\r\nThis project goes beyond a simple model, implementing a robust, professional-grade pipeline using a pre-trained RoBERTa model and a K-Fold Cross-Validation strategy.\r\n\r\n## üèÜ Key Achievements\r\n\r\n  * **Achieved a Top 10 Rank** on the final Kaggle leaderboard out of thousands of participants.\r\n  * Final Kaggle Public Leaderboard Score: **0.84768**.\r\n  * Utilized a robust **5-Fold Cross-Validation** strategy to ensure model stability and performance.\r\n  * Successfully fine-tuned a **RoBERTa-base** model using the Hugging Face and PyTorch ecosystem.\r\n\r\n## üõ†Ô∏è The Technical Pipeline\r\n\r\nThe final, high-scoring solution was achieved through a systematic and robust workflow:\r\n\r\n### 1\\. Data Preprocessing\r\n\r\nInitial cleaning of the tweet text was performed to standardize the input for the model. This included:\r\n\r\n  * Removing all URLs.\r\n  * Stripping HTML tags.\r\n  * Normalizing whitespace.\r\n\r\n### 2\\. Model Architecture: RoBERTa\r\n\r\nThe core of the solution is a pre-trained **RoBERTa-base** model from the Hugging Face `transformers` library. RoBERTa is an optimized version of BERT that is highly effective for language understanding tasks. The model was fine-tuned specifically for this binary classification problem.\r\n\r\n### 3\\. Training Strategy: K-Fold Cross-Validation\r\n\r\nTo build a highly robust model and get a reliable performance estimate, a **Stratified K-Fold** strategy with **5 splits** was implemented.\r\n\r\nThis involves training 5 separate RoBERTa models on different 80% subsets of the training data. Each model is validated on the remaining 20% of the data, ensuring that every sample is used for validation exactly once.\r\n\r\n### 4\\. Final Prediction: Ensembling with Majority Vote\r\n\r\nThe final prediction for each tweet in the test set was determined by a **majority vote** from the 5 models trained during the cross-validation process. This ensembling technique leverages the \"wisdom of the crowd\" by combining the predictions of all models, leading to a more accurate and stable final submission.\r\n\r\n## üìä Results Summary\r\n\r\n  * **Average 5-Fold Validation Accuracy:** **83.61%**\r\n  * **Final Kaggle Public Leaderboard Score:** **0.84768**\r\n  * **Final Kaggle Rank:** **10**\r\n\r\n## Future Improvements\r\n\r\nWhile this solution achieved a top rank, further improvements could be explored:\r\n\r\n  * **Experiment with Larger Models:** Fine-tuning a `roberta-large` model could yield further improvements, though it requires more computational resources.\r\n  * **Advanced Ensembling:** Combining predictions from different transformer architectures (e.g., DeBERTa, BERT, RoBERTa) could create an even more powerful ensemble.\r\n  * **Meticulous Data Cleaning:** A deeper dive into cleaning tweet-specific slang, abbreviations, and misspellings might provide a slight edge.","src/pages/work/nlp_disaster.md","8fc0c9c3768a9df3",{"html":73,"metadata":74},"\u003Ch1 id=\"kaggle-nlp-with-disaster-tweets-top-10-finish\">Kaggle: NLP with Disaster Tweets (Top 10 Finish)\u003C/h1>\n\u003Cp>\u003Ca href=\"https://www.kaggle.com/c/nlp-getting-started\">\u003C/a>\r\n\u003Ca href=\"https://www.kaggle.com/c/nlp-getting-started/leaderboard\">\u003C/a>\r\n\u003Ca href=\"https://www.python.org\">\u003C/a>\r\n\u003Ca href=\"https://pytorch.org/\">\u003C/a>\r\n\u003Ca href=\"https://github.com/huggingface/transformers\">\u003C/a>\u003C/p>\n\u003Ch2 id=\"project-overview\">Project Overview\u003C/h2>\n\u003Cp>This repository contains the code and methodology for the Kaggle competition ‚Äú\u003Ca href=\"https://www.kaggle.com/c/nlp-getting-started\">Natural Language Processing with Disaster Tweets\u003C/a>‚Äù. The goal is to build a machine learning model that can determine whether a given tweet is about a real disaster or not.\u003C/p>\n\u003Cp>This project goes beyond a simple model, implementing a robust, professional-grade pipeline using a pre-trained RoBERTa model and a K-Fold Cross-Validation strategy.\u003C/p>\n\u003Ch2 id=\"-key-achievements\">üèÜ Key Achievements\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Achieved a Top 10 Rank\u003C/strong> on the final Kaggle leaderboard out of thousands of participants.\u003C/li>\n\u003Cli>Final Kaggle Public Leaderboard Score: \u003Cstrong>0.84768\u003C/strong>.\u003C/li>\n\u003Cli>Utilized a robust \u003Cstrong>5-Fold Cross-Validation\u003C/strong> strategy to ensure model stability and performance.\u003C/li>\n\u003Cli>Successfully fine-tuned a \u003Cstrong>RoBERTa-base\u003C/strong> model using the Hugging Face and PyTorch ecosystem.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"Ô∏è-the-technical-pipeline\">üõ†Ô∏è The Technical Pipeline\u003C/h2>\n\u003Cp>The final, high-scoring solution was achieved through a systematic and robust workflow:\u003C/p>\n\u003Ch3 id=\"1-data-preprocessing\">1. Data Preprocessing\u003C/h3>\n\u003Cp>Initial cleaning of the tweet text was performed to standardize the input for the model. This included:\u003C/p>\n\u003Cul>\n\u003Cli>Removing all URLs.\u003C/li>\n\u003Cli>Stripping HTML tags.\u003C/li>\n\u003Cli>Normalizing whitespace.\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"2-model-architecture-roberta\">2. Model Architecture: RoBERTa\u003C/h3>\n\u003Cp>The core of the solution is a pre-trained \u003Cstrong>RoBERTa-base\u003C/strong> model from the Hugging Face \u003Ccode>transformers\u003C/code> library. RoBERTa is an optimized version of BERT that is highly effective for language understanding tasks. The model was fine-tuned specifically for this binary classification problem.\u003C/p>\n\u003Ch3 id=\"3-training-strategy-k-fold-cross-validation\">3. Training Strategy: K-Fold Cross-Validation\u003C/h3>\n\u003Cp>To build a highly robust model and get a reliable performance estimate, a \u003Cstrong>Stratified K-Fold\u003C/strong> strategy with \u003Cstrong>5 splits\u003C/strong> was implemented.\u003C/p>\n\u003Cp>This involves training 5 separate RoBERTa models on different 80% subsets of the training data. Each model is validated on the remaining 20% of the data, ensuring that every sample is used for validation exactly once.\u003C/p>\n\u003Ch3 id=\"4-final-prediction-ensembling-with-majority-vote\">4. Final Prediction: Ensembling with Majority Vote\u003C/h3>\n\u003Cp>The final prediction for each tweet in the test set was determined by a \u003Cstrong>majority vote\u003C/strong> from the 5 models trained during the cross-validation process. This ensembling technique leverages the ‚Äúwisdom of the crowd‚Äù by combining the predictions of all models, leading to a more accurate and stable final submission.\u003C/p>\n\u003Ch2 id=\"-results-summary\">üìä Results Summary\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Average 5-Fold Validation Accuracy:\u003C/strong> \u003Cstrong>83.61%\u003C/strong>\u003C/li>\n\u003Cli>\u003Cstrong>Final Kaggle Public Leaderboard Score:\u003C/strong> \u003Cstrong>0.84768\u003C/strong>\u003C/li>\n\u003Cli>\u003Cstrong>Final Kaggle Rank:\u003C/strong> \u003Cstrong>10\u003C/strong>\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"future-improvements\">Future Improvements\u003C/h2>\n\u003Cp>While this solution achieved a top rank, further improvements could be explored:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Experiment with Larger Models:\u003C/strong> Fine-tuning a \u003Ccode>roberta-large\u003C/code> model could yield further improvements, though it requires more computational resources.\u003C/li>\n\u003Cli>\u003Cstrong>Advanced Ensembling:\u003C/strong> Combining predictions from different transformer architectures (e.g., DeBERTa, BERT, RoBERTa) could create an even more powerful ensemble.\u003C/li>\n\u003Cli>\u003Cstrong>Meticulous Data Cleaning:\u003C/strong> A deeper dive into cleaning tweet-specific slang, abbreviations, and misspellings might provide a slight edge.\u003C/li>\n\u003C/ul>",{"headings":75,"localImagePaths":107,"remoteImagePaths":108,"frontmatter":109,"imagePaths":110},[76,79,82,85,88,92,95,98,101,104],{"depth":29,"slug":77,"text":78},"kaggle-nlp-with-disaster-tweets-top-10-finish","Kaggle: NLP with Disaster Tweets (Top 10 Finish)",{"depth":33,"slug":80,"text":81},"project-overview","Project Overview",{"depth":33,"slug":83,"text":84},"-key-achievements","üèÜ Key Achievements",{"depth":33,"slug":86,"text":87},"Ô∏è-the-technical-pipeline","üõ†Ô∏è The Technical Pipeline",{"depth":89,"slug":90,"text":91},3,"1-data-preprocessing","1. Data Preprocessing",{"depth":89,"slug":93,"text":94},"2-model-architecture-roberta","2. Model Architecture: RoBERTa",{"depth":89,"slug":96,"text":97},"3-training-strategy-k-fold-cross-validation","3. Training Strategy: K-Fold Cross-Validation",{"depth":89,"slug":99,"text":100},"4-final-prediction-ensembling-with-majority-vote","4. Final Prediction: Ensembling with Majority Vote",{"depth":33,"slug":102,"text":103},"-results-summary","üìä Results Summary",{"depth":33,"slug":105,"text":106},"future-improvements","Future Improvements",[],[],{"index":62,"name":63,"logo":64,"featured":64,"role":65,"type":66,"timeline":67,"description":68,"layout":57},[],"wallcrack",{"id":111,"data":113,"body":119,"filePath":120,"digest":121,"rendered":122},{"index":29,"name":114,"logo":115,"featured":115,"role":116,"type":117,"timeline":67,"description":118},"CycleGAN for Crack Image Restoration","/work/gan.svg","Ai Engineer","Deep Learning","This project demonstrates the use of a CycleGAN model to restore degraded images of concrete cracks by removing blur and shadow artifacts. A key component of this work is a detailed quantitative analysis to verify that the restoration process preserves the physical dimensions (width) of the cracks, making the model suitable as a pre-processing step for automated inspection systems.","# CycleGAN for Crack Image Restoration and Feature Preservation Analysis\r\n\r\nThis project demonstrates the use of a CycleGAN model to restore degraded images of concrete cracks by removing blur and shadow artifacts. A key component of this work is a detailed quantitative analysis to verify that the restoration process preserves the physical dimensions (width) of the cracks, making the model suitable as a pre-processing step for automated inspection systems.\r\n\r\n## Visual Demonstration\r\n\r\nThe model is effective at removing both blur and shadow artifacts while maintaining the structural integrity of the original crack.\r\n\r\n\r\n## Methodology\r\n\r\nThe project follows a comprehensive end-to-end pipeline:\r\n\r\n1.  **Data Preparation**: A custom dataset was created by applying blur and shadow augmentations to a set of clean crack images.\r\n2.  **Model Training**: A CycleGAN model was trained on this unpaired dataset to learn the translation from degraded images (Domain A) to clean images (Domain B).\r\n3.  **Performance Evaluation**: The trained model was evaluated using standard image quality metrics (PSNR and SSIM).\r\n4.  **Feature Preservation Analysis**: A detailed analysis, using a custom script based on image skeletonization and distance transforms, was conducted to measure and compare the crack widths before and after restoration.\r\n\r\n## Dataset\r\n\r\nThe model was trained and evaluated on a custom-built dataset derived from a set of 1000 clean, cracked wall images.\r\n\r\n  * **Training Set**:\r\n      * `trainB` (Clean): 800 images.\r\n      * `trainA` (Augmented): 2400 images (800 blurred, 800 shadowed, 800 with both).\r\n  * **Testing Set**:\r\n      * `testB` (Clean Ground Truth): 200 images.\r\n      * `testA` (Augmented): 600 images (200 for each augmentation type).\r\n\r\n### Analysis\r\n\r\nThe final analysis, including statistical calculations and plot generation, is performed using the scripts provided in the `output.ipynb` Jupyter Notebook.\r\n\r\n## Results and Analysis\r\n\r\nThe final model from **Epoch 50** was selected as the best-performing model based on quantitative metrics.\r\n\r\n### Quantitative Model Performance\r\n\r\n| **Average PSNR** | 25.11 dB |\r\n\r\n| **Average SSIM** | 0.8903 |\r\n\r\nThese scores indicate a high level of image quality and structural similarity between the restored images and the original clean images.\r\n\r\n### Crack Width Preservation Analysis\r\n\r\nA detailed analysis was performed to measure the impact of restoration on the physical dimensions of the cracks.\r\n\r\n#### Blur Dataset Analysis\r\n\r\n**Descriptive Statistics:**\r\nThe blur artifact increased the mean measured crack width by over 1mm, while the CycleGAN restored it to near the original value. The model reduced the mean error by \\~70% and the error variance by over 98%.\r\n\r\n\r\n\r\n**Correlation:**\r\nThe heatmap shows a very strong positive correlation (**0.95**) between the clean and restored crack widths, while the correlation with the blurred width was much lower (**0.77**).\r\n\r\n#### Shadow Dataset Analysis\r\n\r\n**Descriptive Statistics:**\r\nThe shadow artifact caused a massive distortion, increasing the mean measured width from \\~2.7mm to \\~14mm. The CycleGAN model successfully corrected this, reducing the mean error by **\\~97%**.\r\n\r\n\r\n\r\n**Correlation and Error Distribution:**\r\nThe visualizations from the report, such as the scatter plot and box plot, provide clear evidence of the model's success. The scatter plot shows a tight clustering of restored widths around the ideal 1:1 line. The box plot demonstrates a dramatic reduction in both the median error and error variance when comparing the shadowed images to the CycleGAN-restored images.\r\n\r\n## Conclusion\r\n\r\nThis project successfully demonstrates that a CycleGAN model can be effectively trained to restore images of concrete cracks degraded by blur and shadow artifacts. The quantitative analysis proves that the model not only improves the visual quality but also preserves the dimensional integrity of the cracks, making it a viable and valuable pre-processing tool for automated structural health monitoring and inspection systems.\r\n\r\n## Acknowledgments\r\n\r\nThis project utilizes code from the `pytorch-CycleGAN-and-pix2pix` repository by Jun-Yan Zhu and Taesung Park.","src/pages/work/wallcrack.md","1bd26d5599d925f9",{"html":123,"metadata":124},"\u003Ch1 id=\"cyclegan-for-crack-image-restoration-and-feature-preservation-analysis\">CycleGAN for Crack Image Restoration and Feature Preservation Analysis\u003C/h1>\n\u003Cp>This project demonstrates the use of a CycleGAN model to restore degraded images of concrete cracks by removing blur and shadow artifacts. A key component of this work is a detailed quantitative analysis to verify that the restoration process preserves the physical dimensions (width) of the cracks, making the model suitable as a pre-processing step for automated inspection systems.\u003C/p>\n\u003Ch2 id=\"visual-demonstration\">Visual Demonstration\u003C/h2>\n\u003Cp>The model is effective at removing both blur and shadow artifacts while maintaining the structural integrity of the original crack.\u003C/p>\n\u003Ch2 id=\"methodology\">Methodology\u003C/h2>\n\u003Cp>The project follows a comprehensive end-to-end pipeline:\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>Data Preparation\u003C/strong>: A custom dataset was created by applying blur and shadow augmentations to a set of clean crack images.\u003C/li>\n\u003Cli>\u003Cstrong>Model Training\u003C/strong>: A CycleGAN model was trained on this unpaired dataset to learn the translation from degraded images (Domain A) to clean images (Domain B).\u003C/li>\n\u003Cli>\u003Cstrong>Performance Evaluation\u003C/strong>: The trained model was evaluated using standard image quality metrics (PSNR and SSIM).\u003C/li>\n\u003Cli>\u003Cstrong>Feature Preservation Analysis\u003C/strong>: A detailed analysis, using a custom script based on image skeletonization and distance transforms, was conducted to measure and compare the crack widths before and after restoration.\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"dataset\">Dataset\u003C/h2>\n\u003Cp>The model was trained and evaluated on a custom-built dataset derived from a set of 1000 clean, cracked wall images.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Training Set\u003C/strong>:\n\u003Cul>\n\u003Cli>\u003Ccode>trainB\u003C/code> (Clean): 800 images.\u003C/li>\n\u003Cli>\u003Ccode>trainA\u003C/code> (Augmented): 2400 images (800 blurred, 800 shadowed, 800 with both).\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\u003Cstrong>Testing Set\u003C/strong>:\n\u003Cul>\n\u003Cli>\u003Ccode>testB\u003C/code> (Clean Ground Truth): 200 images.\u003C/li>\n\u003Cli>\u003Ccode>testA\u003C/code> (Augmented): 600 images (200 for each augmentation type).\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"analysis\">Analysis\u003C/h3>\n\u003Cp>The final analysis, including statistical calculations and plot generation, is performed using the scripts provided in the \u003Ccode>output.ipynb\u003C/code> Jupyter Notebook.\u003C/p>\n\u003Ch2 id=\"results-and-analysis\">Results and Analysis\u003C/h2>\n\u003Cp>The final model from \u003Cstrong>Epoch 50\u003C/strong> was selected as the best-performing model based on quantitative metrics.\u003C/p>\n\u003Ch3 id=\"quantitative-model-performance\">Quantitative Model Performance\u003C/h3>\n\u003Cp>| \u003Cstrong>Average PSNR\u003C/strong> | 25.11 dB |\u003C/p>\n\u003Cp>| \u003Cstrong>Average SSIM\u003C/strong> | 0.8903 |\u003C/p>\n\u003Cp>These scores indicate a high level of image quality and structural similarity between the restored images and the original clean images.\u003C/p>\n\u003Ch3 id=\"crack-width-preservation-analysis\">Crack Width Preservation Analysis\u003C/h3>\n\u003Cp>A detailed analysis was performed to measure the impact of restoration on the physical dimensions of the cracks.\u003C/p>\n\u003Ch4 id=\"blur-dataset-analysis\">Blur Dataset Analysis\u003C/h4>\n\u003Cp>\u003Cstrong>Descriptive Statistics:\u003C/strong>\r\nThe blur artifact increased the mean measured crack width by over 1mm, while the CycleGAN restored it to near the original value. The model reduced the mean error by ~70% and the error variance by over 98%.\u003C/p>\n\u003Cp>\u003Cstrong>Correlation:\u003C/strong>\r\nThe heatmap shows a very strong positive correlation (\u003Cstrong>0.95\u003C/strong>) between the clean and restored crack widths, while the correlation with the blurred width was much lower (\u003Cstrong>0.77\u003C/strong>).\u003C/p>\n\u003Ch4 id=\"shadow-dataset-analysis\">Shadow Dataset Analysis\u003C/h4>\n\u003Cp>\u003Cstrong>Descriptive Statistics:\u003C/strong>\r\nThe shadow artifact caused a massive distortion, increasing the mean measured width from ~2.7mm to ~14mm. The CycleGAN model successfully corrected this, reducing the mean error by \u003Cstrong>~97%\u003C/strong>.\u003C/p>\n\u003Cp>\u003Cstrong>Correlation and Error Distribution:\u003C/strong>\r\nThe visualizations from the report, such as the scatter plot and box plot, provide clear evidence of the model‚Äôs success. The scatter plot shows a tight clustering of restored widths around the ideal 1:1 line. The box plot demonstrates a dramatic reduction in both the median error and error variance when comparing the shadowed images to the CycleGAN-restored images.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>This project successfully demonstrates that a CycleGAN model can be effectively trained to restore images of concrete cracks degraded by blur and shadow artifacts. The quantitative analysis proves that the model not only improves the visual quality but also preserves the dimensional integrity of the cracks, making it a viable and valuable pre-processing tool for automated structural health monitoring and inspection systems.\u003C/p>\n\u003Ch2 id=\"acknowledgments\">Acknowledgments\u003C/h2>\n\u003Cp>This project utilizes code from the \u003Ccode>pytorch-CycleGAN-and-pix2pix\u003C/code> repository by Jun-Yan Zhu and Taesung Park.\u003C/p>",{"headings":125,"localImagePaths":163,"remoteImagePaths":164,"frontmatter":165,"imagePaths":170},[126,129,132,135,138,141,144,147,150,154,157,160],{"depth":29,"slug":127,"text":128},"cyclegan-for-crack-image-restoration-and-feature-preservation-analysis","CycleGAN for Crack Image Restoration and Feature Preservation Analysis",{"depth":33,"slug":130,"text":131},"visual-demonstration","Visual Demonstration",{"depth":33,"slug":133,"text":134},"methodology","Methodology",{"depth":33,"slug":136,"text":137},"dataset","Dataset",{"depth":89,"slug":139,"text":140},"analysis","Analysis",{"depth":33,"slug":142,"text":143},"results-and-analysis","Results and Analysis",{"depth":89,"slug":145,"text":146},"quantitative-model-performance","Quantitative Model Performance",{"depth":89,"slug":148,"text":149},"crack-width-preservation-analysis","Crack Width Preservation Analysis",{"depth":151,"slug":152,"text":153},4,"blur-dataset-analysis","Blur Dataset Analysis",{"depth":151,"slug":155,"text":156},"shadow-dataset-analysis","Shadow Dataset Analysis",{"depth":33,"slug":158,"text":159},"conclusion","Conclusion",{"depth":33,"slug":161,"text":162},"acknowledgments","Acknowledgments",[],[],{"index":29,"name":114,"logo":115,"featured":115,"role":116,"type":117,"timeline":67,"ref_urls":166,"description":118,"layout":57},[167],{"name":168,"url":169},"CycleGAN","https://github.com/Sree14hari/WallCrack-CycleGan.git",[],"emotionai",{"id":171,"data":173,"body":177,"filePath":178,"digest":179,"rendered":180},{"index":33,"name":174,"logo":175,"featured":175,"role":116,"type":117,"timeline":67,"description":176},"Wav2Vec2 Emotion Recognition","/work/xai.svg","Implementing a fine-tuning pipeline for Facebook's Wav2Vec2 model to perform emotion recognition from audio data. The model classifies audio into 21 different emotion categories with varying intensity levels.","# Wav2Vec2 Emotion Recognition Model Analysis\r\n\r\n## Overview\r\n\r\nThis notebook implements a fine-tuning pipeline for Facebook's Wav2Vec2 model to perform emotion recognition from audio data. The model classifies audio into 21 different emotion categories with varying intensity levels.\r\n\r\n## Dataset Structure\r\n\r\n- **Training set**: 2,486 samples\r\n- **Validation set**: 410 samples  \r\n- **Test set**: 738 samples\r\n- **Total**: 3,634 audio files\r\n\r\n## Emotion Labels (21 categories)\r\n\r\nThe model predicts emotions with intensity levels:\r\n\r\n- **ANG** (Anger): HI, LO, MD, XX (High, Low, Medium, Unknown intensity)\r\n- **DIS** (Disgust): HI, LO, MD, XX\r\n- **FEA** (Fear): HI, LO, MD, XX\r\n- **HAP** (Happy): HI, LO, MD, XX\r\n- **NEU** (Neutral): XX only\r\n- **SAD** (Sad): HI, LO, MD, XX\r\n\r\n## Technical Implementation\r\n\r\n### 1. Data Loading and Preprocessing\r\n\r\n- Uses Hugging Face `datasets` library to load CSV files\r\n- Audio resampling to 16kHz (standard for Wav2Vec2)\r\n- Padding/truncation to 5 seconds maximum length\r\n- Label mapping from text to integer IDs\r\n\r\n### 2. Data Augmentation (Training Only)\r\n\r\nUses `audiomentations` library with:\r\n\r\n- **Gaussian Noise**: 0.001-0.015 amplitude (50% probability)\r\n- **Pitch Shift**: ¬±4 semitones (50% probability)\r\n- **Time Stretch**: 0.8-1.25x speed (50% probability)\r\n\r\n### 3. Model Architecture\r\n\r\n- Base model: `facebook/wav2vec2-base`\r\n- Added classification head for 21 emotion classes\r\n- Uses `AutoModelForAudioClassification`\r\n\r\n### 4. Training Configuration\r\n\r\n- Batch size: 4 (both train/eval)\r\n- Learning rate: 1e-5\r\n- Epochs: 15 (interrupted at ~10 epochs)\r\n- Evaluation every 500 steps\r\n- Weight decay: 0.01\r\n- No mixed precision (fp16=False)\r\n\r\n## Training Results Analysis\r\n\r\n### Performance Progression\r\n\r\n| Step | Training Loss | Validation Loss | Accuracy |\r\n|------|---------------|-----------------|----------|\r\n| 500  | 2.539         | 2.360          | 26.6%    |\r\n| 1000 | 2.103         | 1.936          | 38.5%    |\r\n| 3000 | 1.071         | 1.454          | 54.9%    |\r\n| 4500 | 0.731         | 1.608          | 62.4%    |\r\n| 6000 | 0.513         | 1.799          | 61.0%    |\r\n\r\n### Key Observations\r\n\r\n#### ‚úÖ Positive Indicators\r\n\r\n1. **Strong Learning**: Training loss decreased from 2.54 to 0.51 (80% reduction)\r\n2. **Good Initial Progress**: Validation accuracy improved from 26.6% to 62.4%\r\n3. **Reasonable Baseline**: 60% accuracy on 21-class problem (vs 4.8% random chance)\r\n\r\n#### ‚ö†Ô∏è Concerning Patterns\r\n\r\n1. **Overfitting Signs**:\r\n   - Training loss continues decreasing while validation loss increases after step 3000\r\n   - Gap between training and validation loss widens significantly\r\n   - Validation accuracy plateaus around 60%\r\n\r\n2. **Model Convergence Issues**:\r\n   - Validation loss becomes unstable (increases from 1.45 to 1.80)\r\n   - Performance degradation in later steps suggests overtraining\r\n\r\n## Final Test Results\r\n\r\n- **Test Accuracy**: 59.8%\r\n- **Test Loss**: 1.824\r\n\r\n## Recommendations for Improvement\r\n\r\n### 1. Address Overfitting\r\n\r\n- **Early Stopping**: Implement based on validation loss (stop around step 3000)\r\n- **Regularization**: Increase dropout, add L2 regularization\r\n- **Data**: Collect more training samples if possible\r\n\r\n### 2. Hyperparameter Tuning\r\n\r\n- **Learning Rate**: Try 5e-6 or 2e-5\r\n- **Batch Size**: Experiment with larger batches (8, 16) if GPU memory allows\r\n- **Augmentation**: Reduce augmentation intensity or probability\r\n\r\n### 3. Model Architecture\r\n\r\n- **Freeze Layers**: Freeze early Wav2Vec2 layers, only fine-tune later layers\r\n- **Different Base**: Try `wav2vec2-large` or `wav2vec2-large-960h`\r\n\r\n### 4. Data Analysis\r\n\r\n- **Class Balance**: Check for class imbalance issues\r\n- **Data Quality**: Analyze misclassified samples\r\n- **Cross-validation**: Implement k-fold validation\r\n\r\n### 5. Advanced Techniques\r\n\r\n- **Ensemble Methods**: Combine multiple models\r\n- **Pseudo-labeling**: Use model predictions on unlabeled data\r\n- **Multi-task Learning**: Joint training with related tasks\r\n\r\n## Code Quality Assessment\r\n\r\n### Strengths\r\n\r\n- Well-structured pipeline with clear sections\r\n- Proper data cleaning and validation\r\n- Good use of Hugging Face ecosystem\r\n- Comprehensive augmentation strategy\r\n\r\n### Areas for Improvement\r\n\r\n- Missing early stopping implementation\r\n- No hyperparameter search\r\n- Limited error analysis and visualization\r\n- Could benefit from more detailed logging and metrics\r\n\r\n## Conclusion\r\n\r\nThis is a solid implementation that achieves reasonable performance (60% accuracy) on a challenging 21-class emotion recognition task. The main issue is overfitting, which could be addressed with early stopping and regularization techniques. The code demonstrates good understanding of modern NLP/audio processing pipelines.","src/pages/work/emotionai.md","746295b4c692b289",{"html":181,"metadata":182},"\u003Ch1 id=\"wav2vec2-emotion-recognition-model-analysis\">Wav2Vec2 Emotion Recognition Model Analysis\u003C/h1>\n\u003Ch2 id=\"overview\">Overview\u003C/h2>\n\u003Cp>This notebook implements a fine-tuning pipeline for Facebook‚Äôs Wav2Vec2 model to perform emotion recognition from audio data. The model classifies audio into 21 different emotion categories with varying intensity levels.\u003C/p>\n\u003Ch2 id=\"dataset-structure\">Dataset Structure\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Training set\u003C/strong>: 2,486 samples\u003C/li>\n\u003Cli>\u003Cstrong>Validation set\u003C/strong>: 410 samples\u003C/li>\n\u003Cli>\u003Cstrong>Test set\u003C/strong>: 738 samples\u003C/li>\n\u003Cli>\u003Cstrong>Total\u003C/strong>: 3,634 audio files\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"emotion-labels-21-categories\">Emotion Labels (21 categories)\u003C/h2>\n\u003Cp>The model predicts emotions with intensity levels:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>ANG\u003C/strong> (Anger): HI, LO, MD, XX (High, Low, Medium, Unknown intensity)\u003C/li>\n\u003Cli>\u003Cstrong>DIS\u003C/strong> (Disgust): HI, LO, MD, XX\u003C/li>\n\u003Cli>\u003Cstrong>FEA\u003C/strong> (Fear): HI, LO, MD, XX\u003C/li>\n\u003Cli>\u003Cstrong>HAP\u003C/strong> (Happy): HI, LO, MD, XX\u003C/li>\n\u003Cli>\u003Cstrong>NEU\u003C/strong> (Neutral): XX only\u003C/li>\n\u003Cli>\u003Cstrong>SAD\u003C/strong> (Sad): HI, LO, MD, XX\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"technical-implementation\">Technical Implementation\u003C/h2>\n\u003Ch3 id=\"1-data-loading-and-preprocessing\">1. Data Loading and Preprocessing\u003C/h3>\n\u003Cul>\n\u003Cli>Uses Hugging Face \u003Ccode>datasets\u003C/code> library to load CSV files\u003C/li>\n\u003Cli>Audio resampling to 16kHz (standard for Wav2Vec2)\u003C/li>\n\u003Cli>Padding/truncation to 5 seconds maximum length\u003C/li>\n\u003Cli>Label mapping from text to integer IDs\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"2-data-augmentation-training-only\">2. Data Augmentation (Training Only)\u003C/h3>\n\u003Cp>Uses \u003Ccode>audiomentations\u003C/code> library with:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Gaussian Noise\u003C/strong>: 0.001-0.015 amplitude (50% probability)\u003C/li>\n\u003Cli>\u003Cstrong>Pitch Shift\u003C/strong>: ¬±4 semitones (50% probability)\u003C/li>\n\u003Cli>\u003Cstrong>Time Stretch\u003C/strong>: 0.8-1.25x speed (50% probability)\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"3-model-architecture\">3. Model Architecture\u003C/h3>\n\u003Cul>\n\u003Cli>Base model: \u003Ccode>facebook/wav2vec2-base\u003C/code>\u003C/li>\n\u003Cli>Added classification head for 21 emotion classes\u003C/li>\n\u003Cli>Uses \u003Ccode>AutoModelForAudioClassification\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"4-training-configuration\">4. Training Configuration\u003C/h3>\n\u003Cul>\n\u003Cli>Batch size: 4 (both train/eval)\u003C/li>\n\u003Cli>Learning rate: 1e-5\u003C/li>\n\u003Cli>Epochs: 15 (interrupted at ~10 epochs)\u003C/li>\n\u003Cli>Evaluation every 500 steps\u003C/li>\n\u003Cli>Weight decay: 0.01\u003C/li>\n\u003Cli>No mixed precision (fp16=False)\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"training-results-analysis\">Training Results Analysis\u003C/h2>\n\u003Ch3 id=\"performance-progression\">Performance Progression\u003C/h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Step\u003C/th>\u003Cth>Training Loss\u003C/th>\u003Cth>Validation Loss\u003C/th>\u003Cth>Accuracy\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>500\u003C/td>\u003Ctd>2.539\u003C/td>\u003Ctd>2.360\u003C/td>\u003Ctd>26.6%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>1000\u003C/td>\u003Ctd>2.103\u003C/td>\u003Ctd>1.936\u003C/td>\u003Ctd>38.5%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>3000\u003C/td>\u003Ctd>1.071\u003C/td>\u003Ctd>1.454\u003C/td>\u003Ctd>54.9%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>4500\u003C/td>\u003Ctd>0.731\u003C/td>\u003Ctd>1.608\u003C/td>\u003Ctd>62.4%\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>6000\u003C/td>\u003Ctd>0.513\u003C/td>\u003Ctd>1.799\u003C/td>\u003Ctd>61.0%\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Ch3 id=\"key-observations\">Key Observations\u003C/h3>\n\u003Ch4 id=\"-positive-indicators\">‚úÖ Positive Indicators\u003C/h4>\n\u003Col>\n\u003Cli>\u003Cstrong>Strong Learning\u003C/strong>: Training loss decreased from 2.54 to 0.51 (80% reduction)\u003C/li>\n\u003Cli>\u003Cstrong>Good Initial Progress\u003C/strong>: Validation accuracy improved from 26.6% to 62.4%\u003C/li>\n\u003Cli>\u003Cstrong>Reasonable Baseline\u003C/strong>: 60% accuracy on 21-class problem (vs 4.8% random chance)\u003C/li>\n\u003C/ol>\n\u003Ch4 id=\"Ô∏è-concerning-patterns\">‚ö†Ô∏è Concerning Patterns\u003C/h4>\n\u003Col>\n\u003Cli>\n\u003Cp>\u003Cstrong>Overfitting Signs\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Training loss continues decreasing while validation loss increases after step 3000\u003C/li>\n\u003Cli>Gap between training and validation loss widens significantly\u003C/li>\n\u003Cli>Validation accuracy plateaus around 60%\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Model Convergence Issues\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Validation loss becomes unstable (increases from 1.45 to 1.80)\u003C/li>\n\u003Cli>Performance degradation in later steps suggests overtraining\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"final-test-results\">Final Test Results\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>Test Accuracy\u003C/strong>: 59.8%\u003C/li>\n\u003Cli>\u003Cstrong>Test Loss\u003C/strong>: 1.824\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"recommendations-for-improvement\">Recommendations for Improvement\u003C/h2>\n\u003Ch3 id=\"1-address-overfitting\">1. Address Overfitting\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Early Stopping\u003C/strong>: Implement based on validation loss (stop around step 3000)\u003C/li>\n\u003Cli>\u003Cstrong>Regularization\u003C/strong>: Increase dropout, add L2 regularization\u003C/li>\n\u003Cli>\u003Cstrong>Data\u003C/strong>: Collect more training samples if possible\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"2-hyperparameter-tuning\">2. Hyperparameter Tuning\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Learning Rate\u003C/strong>: Try 5e-6 or 2e-5\u003C/li>\n\u003Cli>\u003Cstrong>Batch Size\u003C/strong>: Experiment with larger batches (8, 16) if GPU memory allows\u003C/li>\n\u003Cli>\u003Cstrong>Augmentation\u003C/strong>: Reduce augmentation intensity or probability\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"3-model-architecture-1\">3. Model Architecture\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Freeze Layers\u003C/strong>: Freeze early Wav2Vec2 layers, only fine-tune later layers\u003C/li>\n\u003Cli>\u003Cstrong>Different Base\u003C/strong>: Try \u003Ccode>wav2vec2-large\u003C/code> or \u003Ccode>wav2vec2-large-960h\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"4-data-analysis\">4. Data Analysis\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Class Balance\u003C/strong>: Check for class imbalance issues\u003C/li>\n\u003Cli>\u003Cstrong>Data Quality\u003C/strong>: Analyze misclassified samples\u003C/li>\n\u003Cli>\u003Cstrong>Cross-validation\u003C/strong>: Implement k-fold validation\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"5-advanced-techniques\">5. Advanced Techniques\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Ensemble Methods\u003C/strong>: Combine multiple models\u003C/li>\n\u003Cli>\u003Cstrong>Pseudo-labeling\u003C/strong>: Use model predictions on unlabeled data\u003C/li>\n\u003Cli>\u003Cstrong>Multi-task Learning\u003C/strong>: Joint training with related tasks\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"code-quality-assessment\">Code Quality Assessment\u003C/h2>\n\u003Ch3 id=\"strengths\">Strengths\u003C/h3>\n\u003Cul>\n\u003Cli>Well-structured pipeline with clear sections\u003C/li>\n\u003Cli>Proper data cleaning and validation\u003C/li>\n\u003Cli>Good use of Hugging Face ecosystem\u003C/li>\n\u003Cli>Comprehensive augmentation strategy\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"areas-for-improvement\">Areas for Improvement\u003C/h3>\n\u003Cul>\n\u003Cli>Missing early stopping implementation\u003C/li>\n\u003Cli>No hyperparameter search\u003C/li>\n\u003Cli>Limited error analysis and visualization\u003C/li>\n\u003Cli>Could benefit from more detailed logging and metrics\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>This is a solid implementation that achieves reasonable performance (60% accuracy) on a challenging 21-class emotion recognition task. The main issue is overfitting, which could be addressed with early stopping and regularization techniques. The code demonstrates good understanding of modern NLP/audio processing pipelines.\u003C/p>",{"headings":183,"localImagePaths":256,"remoteImagePaths":257,"frontmatter":258,"imagePaths":263},[184,187,190,193,196,199,202,205,208,211,214,217,220,223,226,229,232,235,238,240,243,246,249,252,255],{"depth":29,"slug":185,"text":186},"wav2vec2-emotion-recognition-model-analysis","Wav2Vec2 Emotion Recognition Model Analysis",{"depth":33,"slug":188,"text":189},"overview","Overview",{"depth":33,"slug":191,"text":192},"dataset-structure","Dataset Structure",{"depth":33,"slug":194,"text":195},"emotion-labels-21-categories","Emotion Labels (21 categories)",{"depth":33,"slug":197,"text":198},"technical-implementation","Technical Implementation",{"depth":89,"slug":200,"text":201},"1-data-loading-and-preprocessing","1. Data Loading and Preprocessing",{"depth":89,"slug":203,"text":204},"2-data-augmentation-training-only","2. Data Augmentation (Training Only)",{"depth":89,"slug":206,"text":207},"3-model-architecture","3. Model Architecture",{"depth":89,"slug":209,"text":210},"4-training-configuration","4. Training Configuration",{"depth":33,"slug":212,"text":213},"training-results-analysis","Training Results Analysis",{"depth":89,"slug":215,"text":216},"performance-progression","Performance Progression",{"depth":89,"slug":218,"text":219},"key-observations","Key Observations",{"depth":151,"slug":221,"text":222},"-positive-indicators","‚úÖ Positive Indicators",{"depth":151,"slug":224,"text":225},"Ô∏è-concerning-patterns","‚ö†Ô∏è Concerning Patterns",{"depth":33,"slug":227,"text":228},"final-test-results","Final Test Results",{"depth":33,"slug":230,"text":231},"recommendations-for-improvement","Recommendations for Improvement",{"depth":89,"slug":233,"text":234},"1-address-overfitting","1. Address Overfitting",{"depth":89,"slug":236,"text":237},"2-hyperparameter-tuning","2. Hyperparameter Tuning",{"depth":89,"slug":239,"text":207},"3-model-architecture-1",{"depth":89,"slug":241,"text":242},"4-data-analysis","4. Data Analysis",{"depth":89,"slug":244,"text":245},"5-advanced-techniques","5. Advanced Techniques",{"depth":33,"slug":247,"text":248},"code-quality-assessment","Code Quality Assessment",{"depth":89,"slug":250,"text":251},"strengths","Strengths",{"depth":89,"slug":253,"text":254},"areas-for-improvement","Areas for Improvement",{"depth":33,"slug":158,"text":159},[],[],{"index":33,"name":174,"logo":175,"featured":175,"role":116,"type":117,"timeline":67,"ref_urls":259,"description":176,"layout":57},[260],{"name":261,"url":262},"X-Ai","https://github.com/Sree14hari/Emotion-Detection-XAi.git",[],"uselessproject",{"id":264,"data":266,"body":270,"filePath":271,"digest":272,"rendered":273},{"index":89,"name":267,"logo":268,"featured":268,"role":65,"type":66,"timeline":67,"description":269},"Useless Project","/work/tinker.svg","VadaScope is an over-engineered AI that settles the age-old debate of what makes a perfect vada. Using a sophisticated CycleGAN model, our system analyzes an image of a vada and provides an objective, unsolicited Vada Perfection Index (VPI) score, ending family arguments one snack at a time.","\u003Cimg width=\"3188\" height=\"1202\" alt=\"frame (3)\" src=\"https://github.com/user-attachments/assets/517ad8e9-ad22-457d-9538-a9e62d137cd7\" />\r\n\r\n# VadaScope [VPI- Vada Perfection Index]üéØ\r\n\r\n## Basic Details\r\n\r\n### Team Name: Super Nova\r\n\r\n### Team Members\r\n\r\n  -Team Lead: Sreehari R - Sree Buddha College Of Engineering, Pattoor\r\n  -Member 2:¬† Abhinav R - Sree Buddha College Of Engineering, Pattoor\r\n\r\n### Project Description\r\n\r\nVadaScope is an over-engineered AI that settles the age-old debate of what makes a perfect vada. Using a sophisticated CycleGAN model, our system analyzes an image of a vada and provides an objective, unsolicited Vada Perfection Index (VPI) score, ending family arguments one snack at a time.\r\n\r\n### The Problem (that doesn't exist)\r\n\r\nEvery day, millions of people consume vadas of questionable quality. The lack of a universal, quantifiable metric for vada perfection‚Äîfrom its roundness to its hole-to-vada ratio‚Äîthreatens the very fabric of our tea-time traditions. This rampant subjectivity is a recipe for culinary chaos and inconsistent snacking experiences.\r\n\r\n### The Solution (that nobody asked for)\r\n\r\nOur solution is a two-stage hybrid pipeline that combines the predictive power of deep learning with the analytical precision of classical computer vision to deliver the ultimate Vada Perfection Index (VPI) score.\r\n\r\n‚ú®Stage 1: The AI Oracle (CycleGAN)\r\nWe start by feeding a real-world vada image into a trained CycleGAN model. Acting as an AI Oracle, the model \"hallucinates\" idealized markings onto the input image ‚Äî essentially generating the platonic ideal of what the vada should look like.\r\n\r\nThe result? A marked image representing the Vada Perfection that could have been.\r\n\r\n‚ú®Stage 2: The Scrutinizer (CV + VPI Calculation)\r\nThe marked image is then passed through a classical Computer Vision module, which extracts four key metrics. These metrics are used to compute the final Vada Perfection Index (VPI\\\u003Csub\\>S\\\u003C/sub\\>) as follows:\r\n\r\nVPI\\_S = (w\\_size ¬∑ S\\_size + w\\_shape ¬∑ S\\_shape + w\\_hole ¬∑ S\\_hole + w\\_color ¬∑ S\\_color) √ó 100\r\n\r\nüìè Metric Breakdown\r\n\r\n| Metric               | Description                                                                                                 |\r\n|----------------------|-------------------------------------------------------------------------------------------------------------|\r\n| **S\u003Csub>shape\u003C/sub>** | Measures how closely the vada's outer boundary matches a perfect circle.                                    |\r\n| **S\u003Csub>hole\u003C/sub>**  | Evaluates the position and roundness of the hole, comparing it with the GAN‚Äôs ideal.                        |\r\n| **S\u003Csub>color\u003C/sub>** | Measures deviation from the perfect golden-brown color using the **CIEDE2000** color-difference formula.     |\r\n| **S\u003Csub>size\u003C/sub>**  | Penalizes vadas that are too big or too small, using a Gaussian function centered on the ideal diameter.     |\r\n\r\n\r\nüéöÔ∏èWeight Calibration\r\nw\\_size¬† = 0.01¬†¬†\r\nw\\_shape = 0.40¬†¬†\r\nw\\_hole¬† = 0.30¬†¬†\r\nw\\_color = 0.29\r\n\r\n## Technical Details\r\n\r\n### Technologies/Components Used\r\n\r\nFor Software:\r\n\r\n  -Languages used: Python\r\n  -Frameworks used: PyTorch, Flask\r\n  -Libraries used:\r\n      - OpenCV (`opencv-python`) for computer vision analysis\r\n      - Pillow (PIL) for image manipulation and report generation\r\n      - Albumentations for data augmentation\r\n      - NumPy for numerical operations\r\n  -Tools used:\r\n      - Git / GitHub for version control\r\n      - Visual Studio Code\r\n\r\nFor Hardware:\r\n\r\n  -Main components: A PC/Laptop with a dedicated NVIDIA GPU. A camera (e.g., phone camera) to capture images of vadas.\r\n  -Specifications: A CUDA-enabled NVIDIA GPU is required for model training (e.g., NVIDIA GeForce RTX 4050 Laptop GPU).\r\n  -Tools required: Not applicable.\r\n\r\n### Project Documentation\r\n\r\nFor Software:\r\n\r\n#### Screenshots\r\n\r\n\u003Cimg width=\"1917\" height=\"990\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0de5b698-972d-4e34-887c-3a7b2a55ae65\" />\r\n\r\n\u003Cimg width=\"1897\" height=\"985\" alt=\"image\" src=\"https://github.com/user-attachments/assets/49ce93ad-848b-4ada-a9c3-ba80bd3e33c5\" />\r\n\r\n\u003Cimg width=\"1200\" height=\"700\" alt=\"report_2b8a4b29f2bd41bda2b41b1817059ab9\" src=\"https://github.com/user-attachments/assets/68c0be8d-04d3-4ed9-a051-5e1a0dd6a13b\" />\r\n\r\n#### Diagrams\r\n\r\n![DIAGRM](https://github.com/user-attachments/assets/7f290296-77fa-4aea-a0c3-24a35c3cc711)\r\n\r\n### Project Demo\r\n\r\n#### Video\r\n\r\nhttps://github.com/user-attachments/assets/0d0acf19-0593-4760-ae34-c46289d867f2\r\n\r\n#### Additional Demos\r\n\r\nhttps://github.com/user-attachments/assets/d4f79607-52d5-4537-8e83-4a740e043168\r\n\r\n## Team Contributions\r\n\r\n  -**Sreehari R**: Led the backend development, including setting up the Flask server, training the CycleGAN model, and implementing the computer vision logic for VPI calculation.\r\n  -**Abhinav R**: Designed and developed the main conceptual idea of the project and the frontend user interface, including the HTML structure, all CSS styling and animations, and the JavaScript logic for handling user interaction and displaying results.\r\n\r\n-----\r\n\r\nMade with ‚ù§Ô∏è at TinkerHub Useless Projects","src/pages/work/uselessproject.md","04bee3bc8aa462a4",{"html":274,"metadata":275},"\u003Cimg width=\"3188\" height=\"1202\" alt=\"frame (3)\" src=\"https://github.com/user-attachments/assets/517ad8e9-ad22-457d-9538-a9e62d137cd7\">\n\u003Ch1 id=\"vadascope-vpi--vada-perfection-index\">VadaScope [VPI- Vada Perfection Index]üéØ\u003C/h1>\n\u003Ch2 id=\"basic-details\">Basic Details\u003C/h2>\n\u003Ch3 id=\"team-name-super-nova\">Team Name: Super Nova\u003C/h3>\n\u003Ch3 id=\"team-members\">Team Members\u003C/h3>\n\u003Cp>-Team Lead: Sreehari R - Sree Buddha College Of Engineering, Pattoor\r\n-Member 2:¬† Abhinav R - Sree Buddha College Of Engineering, Pattoor\u003C/p>\n\u003Ch3 id=\"project-description\">Project Description\u003C/h3>\n\u003Cp>VadaScope is an over-engineered AI that settles the age-old debate of what makes a perfect vada. Using a sophisticated CycleGAN model, our system analyzes an image of a vada and provides an objective, unsolicited Vada Perfection Index (VPI) score, ending family arguments one snack at a time.\u003C/p>\n\u003Ch3 id=\"the-problem-that-doesnt-exist\">The Problem (that doesn‚Äôt exist)\u003C/h3>\n\u003Cp>Every day, millions of people consume vadas of questionable quality. The lack of a universal, quantifiable metric for vada perfection‚Äîfrom its roundness to its hole-to-vada ratio‚Äîthreatens the very fabric of our tea-time traditions. This rampant subjectivity is a recipe for culinary chaos and inconsistent snacking experiences.\u003C/p>\n\u003Ch3 id=\"the-solution-that-nobody-asked-for\">The Solution (that nobody asked for)\u003C/h3>\n\u003Cp>Our solution is a two-stage hybrid pipeline that combines the predictive power of deep learning with the analytical precision of classical computer vision to deliver the ultimate Vada Perfection Index (VPI) score.\u003C/p>\n\u003Cp>‚ú®Stage 1: The AI Oracle (CycleGAN)\r\nWe start by feeding a real-world vada image into a trained CycleGAN model. Acting as an AI Oracle, the model ‚Äúhallucinates‚Äù idealized markings onto the input image ‚Äî essentially generating the platonic ideal of what the vada should look like.\u003C/p>\n\u003Cp>The result? A marked image representing the Vada Perfection that could have been.\u003C/p>\n\u003Cp>‚ú®Stage 2: The Scrutinizer (CV + VPI Calculation)\r\nThe marked image is then passed through a classical Computer Vision module, which extracts four key metrics. These metrics are used to compute the final Vada Perfection Index (VPI&#x3C;sub>S&#x3C;/sub>) as follows:\u003C/p>\n\u003Cp>VPI_S = (w_size ¬∑ S_size + w_shape ¬∑ S_shape + w_hole ¬∑ S_hole + w_color ¬∑ S_color) √ó 100\u003C/p>\n\u003Cp>üìè Metric Breakdown\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Metric\u003C/th>\u003Cth>Description\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>\u003Cstrong>S\u003Csub>shape\u003C/sub>\u003C/strong>\u003C/td>\u003Ctd>Measures how closely the vada‚Äôs outer boundary matches a perfect circle.\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>\u003Cstrong>S\u003Csub>hole\u003C/sub>\u003C/strong>\u003C/td>\u003Ctd>Evaluates the position and roundness of the hole, comparing it with the GAN‚Äôs ideal.\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>\u003Cstrong>S\u003Csub>color\u003C/sub>\u003C/strong>\u003C/td>\u003Ctd>Measures deviation from the perfect golden-brown color using the \u003Cstrong>CIEDE2000\u003C/strong> color-difference formula.\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>\u003Cstrong>S\u003Csub>size\u003C/sub>\u003C/strong>\u003C/td>\u003Ctd>Penalizes vadas that are too big or too small, using a Gaussian function centered on the ideal diameter.\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>üéöÔ∏èWeight Calibration\r\nw_size¬† = 0.01¬†¬†\r\nw_shape = 0.40¬†¬†\r\nw_hole¬† = 0.30¬†¬†\r\nw_color = 0.29\u003C/p>\n\u003Ch2 id=\"technical-details\">Technical Details\u003C/h2>\n\u003Ch3 id=\"technologiescomponents-used\">Technologies/Components Used\u003C/h3>\n\u003Cp>For Software:\u003C/p>\n\u003Cp>-Languages used: Python\r\n-Frameworks used: PyTorch, Flask\r\n-Libraries used:\r\n- OpenCV (\u003Ccode>opencv-python\u003C/code>) for computer vision analysis\r\n- Pillow (PIL) for image manipulation and report generation\r\n- Albumentations for data augmentation\r\n- NumPy for numerical operations\r\n-Tools used:\r\n- Git / GitHub for version control\r\n- Visual Studio Code\u003C/p>\n\u003Cp>For Hardware:\u003C/p>\n\u003Cp>-Main components: A PC/Laptop with a dedicated NVIDIA GPU. A camera (e.g., phone camera) to capture images of vadas.\r\n-Specifications: A CUDA-enabled NVIDIA GPU is required for model training (e.g., NVIDIA GeForce RTX 4050 Laptop GPU).\r\n-Tools required: Not applicable.\u003C/p>\n\u003Ch3 id=\"project-documentation\">Project Documentation\u003C/h3>\n\u003Cp>For Software:\u003C/p>\n\u003Ch4 id=\"screenshots\">Screenshots\u003C/h4>\n\u003Cimg width=\"1917\" height=\"990\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0de5b698-972d-4e34-887c-3a7b2a55ae65\">\n\u003Cimg width=\"1897\" height=\"985\" alt=\"image\" src=\"https://github.com/user-attachments/assets/49ce93ad-848b-4ada-a9c3-ba80bd3e33c5\">\n\u003Cimg width=\"1200\" height=\"700\" alt=\"report_2b8a4b29f2bd41bda2b41b1817059ab9\" src=\"https://github.com/user-attachments/assets/68c0be8d-04d3-4ed9-a051-5e1a0dd6a13b\">\n\u003Ch4 id=\"diagrams\">Diagrams\u003C/h4>\n\u003Cp>\u003Cimg src=\"https://github.com/user-attachments/assets/7f290296-77fa-4aea-a0c3-24a35c3cc711\" alt=\"DIAGRM\">\u003C/p>\n\u003Ch3 id=\"project-demo\">Project Demo\u003C/h3>\n\u003Ch4 id=\"video\">Video\u003C/h4>\n\u003Cp>\u003Ca href=\"https://github.com/user-attachments/assets/0d0acf19-0593-4760-ae34-c46289d867f2\">https://github.com/user-attachments/assets/0d0acf19-0593-4760-ae34-c46289d867f2\u003C/a>\u003C/p>\n\u003Ch4 id=\"additional-demos\">Additional Demos\u003C/h4>\n\u003Cp>\u003Ca href=\"https://github.com/user-attachments/assets/d4f79607-52d5-4537-8e83-4a740e043168\">https://github.com/user-attachments/assets/d4f79607-52d5-4537-8e83-4a740e043168\u003C/a>\u003C/p>\n\u003Ch2 id=\"team-contributions\">Team Contributions\u003C/h2>\n\u003Cp>-\u003Cstrong>Sreehari R\u003C/strong>: Led the backend development, including setting up the Flask server, training the CycleGAN model, and implementing the computer vision logic for VPI calculation.\r\n-\u003Cstrong>Abhinav R\u003C/strong>: Designed and developed the main conceptual idea of the project and the frontend user interface, including the HTML structure, all CSS styling and animations, and the JavaScript logic for handling user interaction and displaying results.\u003C/p>\n\u003Chr>\n\u003Cp>Made with ‚ù§Ô∏è at TinkerHub Useless Projects\u003C/p>",{"headings":276,"localImagePaths":325,"remoteImagePaths":326,"frontmatter":327,"imagePaths":335},[277,280,283,286,289,292,295,298,301,304,307,310,313,316,319,322],{"depth":29,"slug":278,"text":279},"vadascope-vpi--vada-perfection-index","VadaScope [VPI- Vada Perfection Index]üéØ",{"depth":33,"slug":281,"text":282},"basic-details","Basic Details",{"depth":89,"slug":284,"text":285},"team-name-super-nova","Team Name: Super Nova",{"depth":89,"slug":287,"text":288},"team-members","Team Members",{"depth":89,"slug":290,"text":291},"project-description","Project Description",{"depth":89,"slug":293,"text":294},"the-problem-that-doesnt-exist","The Problem (that doesn‚Äôt exist)",{"depth":89,"slug":296,"text":297},"the-solution-that-nobody-asked-for","The Solution (that nobody asked for)",{"depth":33,"slug":299,"text":300},"technical-details","Technical Details",{"depth":89,"slug":302,"text":303},"technologiescomponents-used","Technologies/Components Used",{"depth":89,"slug":305,"text":306},"project-documentation","Project Documentation",{"depth":151,"slug":308,"text":309},"screenshots","Screenshots",{"depth":151,"slug":311,"text":312},"diagrams","Diagrams",{"depth":89,"slug":314,"text":315},"project-demo","Project Demo",{"depth":151,"slug":317,"text":318},"video","Video",{"depth":151,"slug":320,"text":321},"additional-demos","Additional Demos",{"depth":33,"slug":323,"text":324},"team-contributions","Team Contributions",[],[],{"index":89,"name":267,"logo":268,"featured":268,"role":65,"type":66,"timeline":67,"ref_urls":328,"description":269,"layout":57},[329,332],{"name":330,"url":331},"Tinker Hub","https://tinkerhub.org/",{"name":333,"url":334},"Project","https://github.com/Sree14hari/Useless-Project.git",[]]